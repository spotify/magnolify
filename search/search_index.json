{"docs":[{"location":"paradox.json","text":"","title":""},{"location":"index.html","text":"","title":"Magnolify"},{"location":"index.html#magnolify","text":"A collection of Magnolia add-ons for common type class derivation, data type conversion, etc.; a simpler and faster successor to shapeless-datatype.","title":"Magnolify"},{"location":"index.html#getting-help","text":"GitHub discussions","title":"Getting help"},{"location":"index.html#modules","text":"This library includes the following modules.\nmagnolify-avro - conversion between Scala types and Apache Avro GenericRecord magnolify-bigquery - conversion between Scala types and Google Cloud BigQuery TableRow magnolify-bigtable - conversion between Scala types and Google Cloud Bigtable to Mutation, from Row magnolify-cats - type class derivation for Cats, specifically Eq[T] Hash[T] Semigroup[T], CommutativeSemigroup[T], Band[T] Monoid[T], CommutativeMonoid[T] Group[T], CommutativeGroup[T] magnolify-datastore - conversion between Scala types and Google Cloud Datastore Entity magnolify-guava - type class derivation for Guava Funnel[T] magnolify-neo4j - conversion between Scala types and Value magnolify-parquet - support for Parquet columnar storage format. magnolify-protobuf - conversion between Scala types and Google Protocol Buffer Message magnolify-refined - support for simple refinement types from Refined. magnolify-scalacheck - type class derivation for ScalaCheck Arbitrary[T] Cogen[T] magnolify-tensorflow - conversion between Scala types and TensorFlow Example\nComplete type mapping here.","title":"Modules"},{"location":"avro.html","text":"","title":"Avro"},{"location":"avro.html#avro","text":"AvroType[T] provides conversion between Scala type T and Avro GenericRecord. Custom support for type T can be added with an implicit instance of AvroField[T].\nimport java.net.URI\n\ncase class CountryCode(code: String)\ncase class Inner(long: Long, str: String, uri: URI, cc: CountryCode)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\"), CountryCode(\"US\")))\n\nimport magnolify.avro._\nimport org.apache.avro.generic.GenericRecord\n\n// Encode custom type URI as String\nimplicit val uriField: AvroField[URI] = AvroField.from[String](URI.create)(_.toString)\n\n// Encode country code as fixed type\nimplicit val afCountryCode: AvroField[CountryCode] =\n  AvroField.fixed[CountryCode](2)(bs => CountryCode(new String(bs)))(cc => cc.code.getBytes)\n\nval avroType = AvroType[Outer]\nval genericRecord: GenericRecord = avroType.to(record)\nval copy: Outer = avroType.from(genericRecord)\n\n// Avro Schema\nval schema = avroType.schema\nEnum-like types map to Avro enums. See EnumType for more details. Additional AvroField[T] instances for Byte, Char, Short, and UnsafeEnum[T] are available from import magnolify.avro.unsafe._. These conversions are unsafe due to potential overflow.\nAchieving backward compatibility when adding new fields to the case class: new fields must have a default parameter value in order to generate backward compatible Avro schema avroType.schema.\ncase class Record(oldField: String, newField: String = \"\")\n// OR\ncase class Record2(oldField: String, newField: Option[String] = None)\nTo populate Avro type and field docs, annotate the case class and its fields with the @doc annotation.\nimport magnolify.avro._\n\n@doc(\"My record\")\ncase class Record(@doc(\"int field\") i: Int, @doc(\"string field\") s: String)\n\n@doc(\"My enum\")\nobject Color extends Enumeration {\n  type Type = Value\n  val Red, Green, Blue = Value\n}\nThe @doc annotation can also be extended to support custom format.\nimport magnolify.avro._\n\nclass myDoc(doc: String, version: Int) extends doc(s\"doc: $doc, version: $version\")\n\n@myDoc(\"My record\", 2)\ncase class Record(@myDoc(\"int field\", 1) i: Int, @myDoc(\"string field\", 2) s: String)\nTo use a different field case format in target records, add an optional CaseMapper argument to AvroType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.avro._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval avroType = AvroType[LowerCamel](CaseMapper(toSnakeCase))\navroType.to(LowerCamel(\"John\", \"Doe\"))\nAvro decimal and uuid logical types map to BigDecimal and java.util.UUID. Additionally decimal requires precision and optional scale parameter.\nimport magnolify.avro._\n\nimplicit val afBigDecimal: AvroField[BigDecimal] = AvroField.bigDecimal(20, 4)\nAmong the date/time types, date maps to java.time.LocalDate. The other types, timestamp, time and local-timestamp, map to Instant, LocalTime and LocalDateTime in either micro or milliseconds precision with import magnolify.avro.logical.micros._ or import magnolify.avro.logical.millis._.\nMap logical types to BigQuery compatible Avro with import magnolify.avro.logical.bigquery._.","title":"Avro"},{"location":"bigquery.html","text":"","title":"BigQuery"},{"location":"bigquery.html#bigquery","text":"TableRowType[T] provides conversion between Scala type T and BigQuery TableRow. Custom support for type T can be added with an implicit instance of TableRowField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.bigquery._\nimport com.google.api.services.bigquery.model.TableRow\n\n// Encode custom type URI as String\nimplicit val uriField: TableRowField[URI] = TableRowField.from[String](URI.create)(_.toString)\n\nval tableRowType = TableRowType[Outer]\nval tableRow: TableRow = tableRowType.to(record)\nval copy: Outer = tableRowType.from(tableRow)\n\n// BigQuery TableSchema\nval schema = tableRowType.schema\nAdditional TableRowField[T] instances for Byte, Char, Short, Int, Float, and enum-like types are available from import magnolify.bigquery.unsafe._. These conversions are unsafe due to potential overflow or encoding errors. See EnumType for more details.\nTo populate BigQuery table and field descriptions, annotate the case class and its fields with the @description annotation.\nimport magnolify.bigquery._\n\n@description(\"My record\")\ncase class Record(@description(\"int field\") i: Int, @description(\"string field\") s: String)\nThe @description annotation can also be extended to support custom format.\nimport magnolify.bigquery._\n\nclass myDesc(description: String, version: Int)\n  extends description(s\"description: $description, version: $version\")\n\n@myDesc(\"My record\", 2)\ncase class Record(@myDesc(\"int field\", 1) i: Int, @myDesc(\"string field\", 2) s: String)\nTo use a different field case format in target records, add an optional CaseMapper argument to TableRowType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.bigquery._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval tableRowType = TableRowType[LowerCamel](CaseMapper(toSnakeCase))\ntableRowType.to(LowerCamel(\"John\", \"Doe\"))","title":"BigQuery"},{"location":"bigtable.html","text":"","title":"Bigtable"},{"location":"bigtable.html#bigtable","text":"BigtableType[T] provides conversion between Scala type T and Bigtable Row/Seq[Mutation] for read/write. Custom support for type T can be added with an implicit instance of BigtableField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.bigtable._\nimport com.google.bigtable.v2.{Mutation, Row}\nimport com.google.protobuf.ByteString\n\n// Encode custom type URI as String\nimplicit val uriField: BigtableField[URI] = BigtableField.from[String](URI.create)(_.toString)\n\nval bigtableType = BigtableType[Outer]\nval mutations: Seq[Mutation] = bigtableType(record, \"ColumnFamily\")\nval row: Row = BigtableType.mutationsToRow(ByteString.copyFromUtf8(\"RowKey\"), mutations)\nval copy: Outer = bigtableType(row, \"ColumnFamily\")\nBigtableType encodes each field in a separate column qualifier of the same name. It encodes nested fields by joining field names as field_a.field_b.field_c. Repeated types are not supported. Enum-like types map to strings. See EnumType for more details.\nTo use a different field case format in target records, add an optional CaseMapper argument to BigtableType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.bigtable._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval bigtableType = BigtableType[LowerCamel](CaseMapper(toSnakeCase))\nbigtableType(LowerCamel(\"John\", \"Doe\"), \"cf\")","title":"Bigtable"},{"location":"cats.html","text":"","title":"Cats"},{"location":"cats.html#cats","text":"Type class derivation for Cats can be performed both automatically and semi-automatically.\nAutomatic derivation are provided as implicits through import magnolify.cats.auto._.\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\n// Cats Semigroup\nimport magnolify.cats.auto._\nimport cats._\n\nval sg: Semigroup[Outer] = implicitly[Semigroup[Outer]]\nsg.combine(Outer(Inner(1, \"hello, \")), Outer(Inner(100, \"world!\"))) // = Outer(Inner(101,hello, world!))\nSome Algebird instances extend those from Cats and can be derived as well.\nimport magnolify.cats.auto._\nimport com.twitter.{algebird => a}\n\ncase class Record(i: Int, o: Option[Int], l: List[Int], s: Set[Int], m: Map[String, Int])\n\n// implicit conversion from cats.{Semigroup,Monoid} to com.twitter.algebird.{Semigroup,Monoid}\nval sg: a.Semigroup[Record] = implicitly[a.Semigroup[Record]]\nval mon: a.Monoid[Record] = implicitly[a.Monoid[Record]]\nSemi-automatic derivation needs to be called explicitly.\nimport magnolify.cats.semiauto._\nimport cats._\n\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\nval eq: Eq[Outer] = EqDerivation[Outer]\nval hash: Hash[Outer] = HashDerivation[Outer]\nval sg: Semigroup[Outer] = SemigroupDerivation[Outer]\nval mon: Monoid[Outer] = MonoidDerivation[Outer]\n\n// this fails due to missing `Group[String]` instance\nval group: Group[Outer] = GroupDerivation[Outer]","title":"Cats"},{"location":"datastore.html","text":"","title":"Datastore"},{"location":"datastore.html#datastore","text":"EntityType[T] provides conversion between Scala type T and Datastore Entity. Custom support for type T can be added with an implicit instance of EntityField[T].\nimport java.net.URI\n\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.datastore._\nimport com.google.datastore.v1.Entity\n\n// Encode custom type URI as String\nimplicit val uriField: EntityField[URI] = EntityField.from[String](URI.create)(_.toString)\n\nval entityType = EntityType[Outer]\nval entityBuilder: Entity.Builder = entityType.to(record)\nval copy: Outer = entityType.from(entityBuilder.build)\nAdditional EntityField[T] instances for Byte, Char, Short, Int, Float, and enum-like types are available from import magnolify.datastore.unsafe._. These conversions are unsafe due to potential overflow or encoding errors. Enum like types map to strings. See EnumType for more details.\nTo set a field as key, annotate the field with key annotation.\nimport magnolify.datastore._\nimport com.google.protobuf.ByteString\n\n// Leave projectId as empty, use current package as namespaceId and class name \"Record\" as kind\ncase class Record(@key k: String, v: Long)\n\n// Custom projectId, namespaceId and kind.\ncase class ExplicitRecord(@key(\"my-project\", \"com.spotify\", \"MyKind\") k: String, v: Long)\n\n// Encode custom key type ByteString as String\ncase class ByteStringKey(@key k: ByteString)\nimplicit val kfByteString: KeyField[ByteString] = KeyField.at[ByteString](_.toStringUtf8)\n\n// Encode custom key type RecordKey as String\ncase class RecordKey(s: String, l: Long)\ncase class NestedKey(@key k: RecordKey)\nimplicit val kfRecord: KeyField[RecordKey] = KeyField.at[RecordKey](r => r.s + r.l)\nTo exclude a property from indexes, annotate the field with excludeFromIndexes annotation.\nimport magnolify.datastore._\ncase class Record(@excludeFromIndexes i: Int, @excludeFromIndexes(true) s: String)\nTo use a different field case format in target records, add an optional CaseMapper argument to EntityType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.datastore._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval entityType = EntityType[LowerCamel](CaseMapper(toSnakeCase))\nentityType.to(LowerCamel(\"John\", \"Doe\"))","title":"Datastore"},{"location":"guava.html","text":"","title":"Guava"},{"location":"guava.html#guava","text":"Type class derivation for Guava can be performed both automatically and semi-automatically.\nAutomatic derivation are provided as implicits through import magnolify.guava.auto._.\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\n// Guava Funnel\nimport magnolify.guava.auto._ // includes implicit instances for Funnel[Int], etc.\nimport com.google.common.hash._\n\nval fnl: Funnel[Outer] = implicitly[Funnel[Outer]]\nval bf: BloomFilter[Outer] = BloomFilter.create[Outer](fnl, 1000)\nSemi-automatic derivation needs to be called explicitly.\nimport magnolify.guava.semiauto._\nimport com.google.common.hash._\n\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\nval fnl: Funnel[Outer] = FunnelDerivation[Outer]","title":"Guava"},{"location":"neo4j.html","text":"","title":"Neo4j"},{"location":"neo4j.html#neo4j","text":"TODO","title":"Neo4j"},{"location":"parquet.html","text":"","title":"Parquet"},{"location":"parquet.html#parquet","text":"ParquetType[T] provides read and write support between Scala type T and the Parquet columnar storage format. Custom support for type T can be added with an implicit instance of ParquetField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.parquet._\n\n// Encode custom type URI as String\nimplicit val uriField: ParquetField[URI] = ParquetField.from[String](b => URI.create(b))(_.toString)\n\nval parquetType = ParquetType[Outer]\n\n// Parquet schema\nval schema = parquetType.schema\nUse ParquetType#readBuilder and ParquetType#writeBuilder to create new file reader and writer instances. See HadoopSuite.scala for examples with Hadoop IO.","title":"Parquet"},{"location":"parquet.html#case-mapping","text":"To use a different field case format in target records, add an optional CaseMapper argument to ParquetType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\nimport magnolify.parquet._\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval parquetType = ParquetType[LowerCamel](CaseMapper(toSnakeCase))","title":"Case Mapping"},{"location":"parquet.html#enums","text":"Enum-like types map to strings. See EnumType for more details. Additional ParquetField[T] instances for Char and UnsafeEnum[T] are available from import magnolify.parquet.unsafe._. This conversions is unsafe due to potential overflow.","title":"Enums"},{"location":"parquet.html#logical-types","text":"Parquet decimal logical type maps to BigDecimal and supports the following encodings:\nimport magnolify.parquet._\n\nval pfDecimal32 = ParquetField.decimal32(9, 0)\nval pfDecimal64 = ParquetField.decimal64(18, 0)\nval pfDecimalFixed = ParquetField.decimalFixed(8, 18, 0)\nval pfDecimalBinary = ParquetField.decimalBinary(20, 0)\nFor a full specification of Date/Time mappings in Parquet, see Type Mappings.","title":"Logical Types"},{"location":"parquet.html#avro-compatibility","text":"The official Parquet format specification supports the REPEATED modifier to denote array types. By default, magnolify-parquet conforms to this specification:\nimport magnolify.parquet._\n\ncase class MyRecord(listField: List[Int])\nParquetType[MyRecord].schema\n// res3: org.apache.parquet.schema.MessageType = message repl.MdocSession.MdocApp.MyRecord {\n//   repeated int32 listField (INTEGER(32,true));\n// }\n//\nHowever, the parquet-avro API encodes array types differently: as a nested array inside a required group.\nimport org.apache.avro.Schema\nval avroSchema = new Schema.Parser().parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"MyRecord\\\",\\\"fields\\\":[{\\\"name\\\": \\\"listField\\\", \\\"type\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": \\\"string\\\"}}]}\")\n// avroSchema: Schema = {\"type\":\"record\",\"name\":\"MyRecord\",\"fields\":[{\"name\":\"listField\",\"type\":{\"type\":\"array\",\"items\":\"string\"}}]}\n\nimport org.apache.parquet.avro.AvroSchemaConverter\nnew AvroSchemaConverter().convert(avroSchema)\n// res4: org.apache.parquet.schema.MessageType = message MyRecord {\n//   required group listField (LIST) {\n//     repeated binary array (STRING);\n//   }\n// }\n//\nDue to this discrepancy, by default, a Repeated type (i.e. a List or Seq) written by parquet-avro isn’t readable by magnolify-parquet, and vice versa.\nTo address this, magnolify-parquet supports an “Avro compatibility mode” that, when enabled, will:\nUse the same Repeated schema format as parquet-avro Write an additional metadata key, parquet.avro.schema, to the Parquet file footer, containing the equivalent Avro schema.","title":"Avro Compatibility"},{"location":"parquet.html#enabling-avro-compatibility-mode","text":"You can enable this mode by importing magnolify.parquet.ParquetArray.AvroCompat._:\nimport magnolify.parquet._\nimport magnolify.parquet.ParquetArray.AvroCompat._\n\ncase class MyRecord(listField: List[Int])\n// List schema matches parquet-avro spec\nParquetType[MyRecord].schema\n// res6: org.apache.parquet.schema.MessageType = message repl.MdocSession.MdocApp5.MyRecord {\n//   required group listField (LIST) {\n//     repeated int32 array (INTEGER(32,true));\n//   }\n// }\n// \n\n// This String value of this schema will be written to the Parquet metadata key `parquet.avro.schema`\nParquetType[MyRecord].avroSchema\n// res7: org.apache.avro.Schema = {\"type\":\"record\",\"name\":\"MyRecord\",\"namespace\":\"repl.MdocSession.MdocApp5\",\"fields\":[{\"name\":\"listField\",\"type\":{\"type\":\"array\",\"items\":\"int\"}}]}","title":"Enabling Avro Compatibility Mode"},{"location":"parquet.html#field-descriptions","text":"The top level class and all fields (including nested class fields) can be annotated with @doc annotation. Note that nested classes annotations are ignored.\nimport magnolify.shared._\n\n@doc(\"This is ignored\")\ncase class NestedClass(@doc(\"nested field annotation\") i: Int)\n\n@doc(\"Top level annotation\")\ncase class TopLevelType(@doc(\"field annotation\") pd: NestedClass)\nNote that field descriptions are not natively supported by the Parquet format. Instead, the @doc annotation ensures that in Avro compat mode, the generated Avro schema written to the metadata key parquet.avro.schema will contain the specified field description:\nimport magnolify.parquet._\n// AvroCompat is required to write `parquet.avro.schema` key to file metadata\nimport magnolify.parquet.ParquetArray.AvroCompat._\nimport magnolify.shared._\n\n@doc(\"Top level annotation\")\ncase class MyRecord(@doc(\"field annotation\") listField: List[Int])\n\nval writer = ParquetType[MyRecord]\n  .writeBuilder(HadoopOutputFile.fromPath(path, new Configuration()))\n  .build()\n// writer: org.apache.parquet.hadoop.ParquetWriter[MyRecord] = org.apache.parquet.hadoop.ParquetWriter@764b7ce6\nwriter.write(MyRecord(List(1,2,3)))\nwriter.close()\n\n// Note that Parquet MessageType schema doesn't contain descriptor, but serialized Avro schema does\nParquetFileReader.open(HadoopInputFile.fromPath(path, new Configuration())).getFileMetaData\n// res12: org.apache.parquet.hadoop.metadata.FileMetaData = FileMetaData{schema: message repl.MdocSession.MdocApp9.MyRecord {\n//   required group listField (LIST) {\n//     repeated int32 array (INTEGER(32,true));\n//   }\n// }\n// , metadata: {writer.model.name=magnolify, parquet.avro.schema={\"type\":\"record\",\"name\":\"MyRecord\",\"namespace\":\"repl.MdocSession.MdocApp9\",\"doc\":\"Top level annotation\",\"fields\":[{\"name\":\"listField\",\"type\":{\"type\":\"array\",\"items\":\"int\"},\"doc\":\"field annotation\"}]}}}\nTherefore, enabling Avro compatibility mode via the AvroCompat import is required to use the @doc annotation with ParquetType.","title":"Field Descriptions"},{"location":"protobuf.html","text":"","title":"Protobuf"},{"location":"protobuf.html#protobuf","text":"ProtobufType[T, MsgT] provides conversion between Scala type T and Protobuf MsgT <: Message. Custom support for type T can be added with an implicit instance of ProtobufField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\n// Protobuf record\nabstract class MyProto extends com.google.protobuf.Message\n\nimport magnolify.protobuf._\n\n// Encode custom type URI as String\nimplicit val uriField = ProtobufField.from[String](URI.create)(_.toString)\n \n// MyProto is a compiled Protobuf Message\nval protobufType = ProtobufType[Outer, MyProto]\nval proto: MyProto = protobufType.to(record)\nval copy: Outer = protobufType.from(proto)\nEnum like types map to Protobuf enums. See EnumType for more details. An implicit instance from Java or Scala type to Protobuf enum must be provided.\n// Scala enum\nobject Color extends Enumeration {\n  type Type = Value\n  val Red, Green, Blue = Value\n}\n\n// Protobuf enum\n// enum ColorProto {\n//   RED = 0;\n//   GREEN = 1;\n//   BLUE = 2;\n// }\nabstract class ColorProto(name: String, ordinal: Int) extends Enum[ColorProto](name, ordinal) with com.google.protobuf.ProtocolMessageEnum\n\nimport magnolify.protobuf._\nimplicit val efEnum = ProtobufField.enum[Color.Type, ColorProto]\nAdditional ProtobufField[T] instances for Byte, Char, Short, and UnsafeEnum[T] are available from import magnolify.protobuf.unsafe._. These conversions are unsafe due to potential overflow.\nTo use a different field case format in target records, add an optional CaseMapper argument to ProtobufType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\nimport magnolify.protobuf._\n\ncase class LowerCamel(firstName: String, lastName: String)\n\n// Protobuf record\nabstract class LowerHyphenProto extends com.google.protobuf.Message\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval protobufType = ProtobufType[LowerCamel, LowerHyphenProto](CaseMapper(toSnakeCase))\nprotobufType.to(LowerCamel(\"John\", \"Doe\"))","title":"Protobuf"},{"location":"refined.html","text":"","title":"Refined"},{"location":"refined.html#refined","text":"Refined is a Scala library for refining types with type-level predicates which constrain the set of values described by the refined type.\nimport eu.timepit.refined._\nimport eu.timepit.refined.api._\nimport eu.timepit.refined.auto._\nimport eu.timepit.refined.numeric._\nimport eu.timepit.refined.string._\n\ncase class Record(pos: Int Refined Positive, url: String Refined Url)\n\n// Refinements are checked at compile time with literals\nRecord(42, \"https://www.spotify.com\")\n// otherwise unsafe runtime check have to be used for variables.\nval x = -1\nval url = \"foo\"\n// Throws IllegalArgumentException\nRecord(refineV.unsafeFrom(x), refineV.unsafeFrom(url))\nMagnolify works with Refined through some extra implicits.\nimport eu.timepit.refined.api._\nimport eu.timepit.refined.auto._\nimport eu.timepit.refined.numeric._\nimport eu.timepit.refined.string._\n\nimport magnolify.avro._\nimport magnolify.refined.avro._\n\ncase class Record(pos: Int Refined Positive, url: String Refined Url)\n\nval at = AvroType[Record]\nat(Record(42, \"https://www.spotify.com\"))","title":"Refined"},{"location":"scalacheck.html","text":"","title":"Scalacheck"},{"location":"scalacheck.html#scalacheck","text":"Type class derivation for Scalacheck can be performed both automatically and semi-automatically.\nAutomatic derivation are provided as implicits through import magnolify.scalacheck.auto._.\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\n// ScalaCheck Arbitrary\nimport magnolify.scalacheck.auto._\nimport org.scalacheck._ // implicit instances for Arbitrary[Int], etc.\n\nval arb: Arbitrary[Outer] = implicitly[Arbitrary[Outer]]\narb.arbitrary.sample // = Some(Outer(Inter(12345, abcde)))\nSemi-automatic derivation needs to be called explicitly.\nimport magnolify.scalacheck.semiauto._\nimport org.scalacheck._\n\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\nval arb: Arbitrary[Outer] = ArbitraryDerivation[Outer]\nval cogen: Cogen[Outer] = CogenDerivation[Outer]","title":"Scalacheck"},{"location":"tensorflow.html","text":"","title":"Tensorflow"},{"location":"tensorflow.html#tensorflow","text":"ExampleType[T] provides conversion between Scala type T and TensorFlow Example. Custom support for type T can be added with an implicit instance of ExampleField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.tensorflow._\nimport com.google.protobuf.ByteString\nimport org.tensorflow.proto.example.{Example, Example.Builder}\n\n// Encode custom type String/URI as ByteString\nimplicit val stringField = ExampleField.from[ByteString](_.toStringUtf8)(ByteString.copyFromUtf8)\nimplicit val uriField = ExampleField\n  .from[ByteString](b => URI.create(b.toStringUtf8))(u => ByteString.copyFromUtf8(u.toString))\n\nval exampleType = ExampleType[Outer]\nval exampleBuilder: Example.Builder = exampleType.to(record)\nval copy = exampleType.from(exampleBuilder.build)\nExampleType encodes each field in a Feature of the same name. It encodes nested fields by joining field names as field_a.field_b.field_c. Optional and repeated types are not supported in a nested field.\nAdditional ExampleField[T] instances for Byte, Char, Short, Int, Double, Boolean, String, and enum-like types are available from import magnolify.tensorflow.unsafe._. These conversions are unsafe due to potential overflow and encoding errors. Enum-like types map to strings. See EnumType for more details.\nTo use a different field case format in target records, add an optional CaseMapper argument to ExampleType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval exampleType = ExampleType[LowerCamel](CaseMapper(toSnakeCase))\nexampleType.to(LowerCamel(\"John\", \"Doe\"))","title":"Tensorflow"},{"location":"enums.html","text":"","title":"EnumType"},{"location":"enums.html#enumtype","text":"EnumType[T] provides conversion between enum-like types and their string names. Supported enum-like types are Java enum, Scala Enumeration, and sealed trait with case objects. AvroType[T] and ProtobufType[T] use it to map to their native enum types, i.e. EnumSymbol and ProtocolMessageEnum, while other converters map to strings.\nCaseMapper supports enums too.\nobject Color extends Enumeration {\n  type Type = Value\n  val Red, Green, Blue = Value\n}\n\nimport magnolify.shared._\n// Encode as [\"red\", \"green\", \"blue\"]\nimplicit val enumType: EnumType[Color.Type] = EnumType.scalaEnumType[Color.Type].map(CaseMapper(_.toLowerCase))\n// enumType: EnumType[Color.Type] = magnolify.shared.EnumType$$anon$1@53f63b7a\nAn enum-like type can be wrapped inside a magnolify.shared.UnsafeEnum[T] to handle unknown cases. This could be useful for scenarios like schema evolution, or working with bad data.\nUnsafeEnum(Color.Red) // Known(Red)\n// res0: UnsafeEnum.Known[Color.Value] = Known(value = Red)\nUnsafeEnum.from[Color.Type](\"Red\") // Known(Red)\n// res1: UnsafeEnum[Color.Type] = Unknown(value = \"Red\")\nUnsafeEnum.from[Color.Type](\"Purple\") // Unknown(Purple)\n// res2: UnsafeEnum[Color.Type] = Unknown(value = \"Purple\")","title":"EnumType"},{"location":"mapping.html","text":"","title":"Type Mapping"},{"location":"mapping.html#type-mapping","text":"Scala Avro BigQuery Bigtable7 Datastore Parquet Protobuf TensorFlow Unit null x x Null x x x Boolean boolean BOOL Byte Boolean BOOLEAN Boolean INT643 Char int3 INT643 Char Integer3 INT323 Int3 INT643 Byte int3 INT643 Byte Integer3 INT329 Int3 INT643 Short int3 INT643 Short Integer3 INT329 Int3 INT643 Int int INT643 Int Integer3 INT329 Int INT643 Long long INT64 Long Integer INT649 Long INT64 Float float FLOAT643 Float Double3 FLOAT Float FLOAT Double double FLOAT64 Double Double DOUBLE Double FLOAT3 CharSequence string x x x x x x String string STRING String String BINARY String BYTES3 Array[Byte] bytes BYTES ByteString Blob BINARY ByteString BYTES ByteString x x ByteString Blob x ByteString BYTES ByteBuffer bytes x x x x x Enum1 enum STRING3 String String3 BINARY/ENUM9 Enum BYTES3 BigInt x x BigInt x x x x BigDecimal bytes4 NUMERIC6 Int scale + unscaled BigInt x LOGICAL[DECIMAL]9,14 x x Option[T] union[null, T]5 NULLABLE Empty as None Absent as None OPTIONAL optional10 Size <= 1 Iterable[T]2 array[T] REPEATED x Array REPEATED13 repeated Size >= 0 Nested record STRUCT Flat8 Entity Group Message Flat8 Map[K, V] map[V]15 x x x x map<K, V> x java.time.Instant long11 TIMESTAMP x Timestamp LOGICAL[TIMESTAMP]9 x x java.time.LocalDateTime long11 DATETIME x x LOGICAL[TIMESTAMP]9 x x java.time.OffsetTime x x x x LOGICAL[TIME]9 x x java.time.LocalTime long11 TIME x x LOGICAL[TIME]9 x x java.time.LocalDate int11 DATE x x LOGICAL[DATE]9 x x org.joda.time.LocalDate int11 x x x x x x org.joda.time.DateTime int11 x x x x x x org.joda.time.LocalTime int11 x x x x x x java.util.UUID string4 x ByteString (16 bytes) x FIXED[16] x x (Long, Long, Long)12 fixed[12] x x x x x x\nThose wrapped inUnsafeEnum are encoded as strings, see enums.md for more Any subtype of Iterable[T] Unsafe conversions, import magnolify.$MODULE.unsafe._ Avro logical types (doc) UNION of [NULL, T] and defaults to NULL (doc) Fixed precision of 38 and scale of 9 (doc) All Scala types are encoded as big endian ByteString for Bigtable Nested fields are encoded flat with field names joined with ., e.g. level1.level2.level3 More information on Parquet logical type schemas can be found here. Time types are available at multiple precisions; import magnolify.parquet.logical.micros._, magnolify.avro.logical.millis._, or magnolify.avro.logical.nanos._ accordingly. See protobuf.md for more Logical types available at micro- or milli-second precision; import magnolify.avro.logical.micros._ or magnolify.avro.logical.millis._ accordingly. BigQuery-compatible conversions are available in magnolify.avro.logical.bigquery._. Special tuple used to represent Duration in the Avro spec. This has not been made implicit in Magnolify; import AvroType.afDuration implicitly to enable If magnolify.parquet.ParquetArray.AvroCompat._ is imported, array fields use the nested, Avro-compatible schema format: required group $FIELDNAME (LIST) { repeated $FIELDTYPE array ($FIELDSCHEMA); }. Parquet’s Decimal logical format supports multiple representations, and are not implicitly scoped by default. Import one of: magnolify.parquet.ParquetField.{decimal32, decimal64, decimalFixed, decimalBinary}. Map key type in avro is fixed to string. Scala Map key type must be either String or CharSequence.","title":"Type Mapping"},{"location":"scaladoc.html","text":"","title":""}]}