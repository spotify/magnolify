{"docs":[{"location":"paradox.json","text":"","title":""},{"location":"index.html","text":"","title":"Magnolify"},{"location":"index.html#magnolify","text":"A collection of Magnolia add-ons for common type class derivation, data type conversion, etc.; a simpler and faster successor to shapeless-datatype.","title":"Magnolify"},{"location":"index.html#getting-help","text":"GitHub discussions","title":"Getting help"},{"location":"index.html#modules","text":"This library includes the following modules.\nmagnolify-avro - conversion between Scala types and Apache Avro GenericRecord magnolify-beam - conversion between Scala types and Apache Beam schema types magnolify-bigquery - conversion between Scala types and Google Cloud BigQuery TableRow magnolify-bigtable - conversion between Scala types and Google Cloud Bigtable to Mutation, from Row magnolify-cats - type class derivation for Cats, specifically Eq[T] Hash[T] Semigroup[T], CommutativeSemigroup[T], Band[T] Monoid[T], CommutativeMonoid[T] Group[T], CommutativeGroup[T] magnolify-datastore - conversion between Scala types and Google Cloud Datastore Entity magnolify-guava - type class derivation for Guava Funnel[T] magnolify-neo4j - conversion between Scala types and Value magnolify-parquet - support for Parquet columnar storage format. magnolify-protobuf - conversion between Scala types and Google Protocol Buffer Message magnolify-refined - support for simple refinement types from Refined. magnolify-scalacheck - type class derivation for ScalaCheck Arbitrary[T] Cogen[T] magnolify-tensorflow - conversion between Scala types and TensorFlow Example\nComplete type mapping here.","title":"Modules"},{"location":"avro.html","text":"","title":"Avro"},{"location":"avro.html#avro","text":"AvroType[T] provides conversion between Scala type T and Avro GenericRecord. Custom support for type T can be added with an implicit instance of AvroField[T].\nimport java.net.URI\n\ncase class CountryCode(code: String)\ncase class Inner(long: Long, str: String, uri: URI, cc: CountryCode)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\"), CountryCode(\"US\")))\n\nimport magnolify.avro._\nimport org.apache.avro.generic.GenericRecord\n\n// Encode custom type URI as String\nimplicit val uriField: AvroField[URI] = AvroField.from[String](URI.create)(_.toString)\n\n// Encode country code as fixed type\nimplicit val afCountryCode: AvroField[CountryCode] =\n  AvroField.fixed[CountryCode](2)(bs => CountryCode(new String(bs)))(cc => cc.code.getBytes)\n\nval avroType = AvroType[Outer]\nval genericRecord: GenericRecord = avroType.to(record)\nval copy: Outer = avroType.from(genericRecord)\n\n// Avro Schema\nval schema = avroType.schema\nEnum-like types map to Avro enums. See EnumType for more details. Additional AvroField[T] instances for Byte, Char, Short, and UnsafeEnum[T] are available from import magnolify.avro.unsafe._. These conversions are unsafe due to potential overflow.\nAchieving backward compatibility when adding new fields to the case class: new fields must have a default parameter value in order to generate backward compatible Avro schema avroType.schema.\ncase class Record(oldField: String, newField: String = \"\")\n// OR\ncase class Record2(oldField: String, newField: Option[String] = None)\nTo populate Avro type and field docs, annotate the case class and its fields with the @doc annotation.\nimport magnolify.avro._\n\n@doc(\"My record\")\ncase class Record(@doc(\"int field\") i: Int, @doc(\"string field\") s: String)\n\n@doc(\"My enum\")\nobject Color extends Enumeration {\n  type Type = Value\n  val Red, Green, Blue = Value\n}\nThe @doc annotation can also be extended to support custom format.\nimport magnolify.avro._\n\nclass myDoc(doc: String, version: Int) extends doc(s\"doc: $doc, version: $version\")\n\n@myDoc(\"My record\", 2)\ncase class Record(@myDoc(\"int field\", 1) i: Int, @myDoc(\"string field\", 2) s: String)\nTo use a different field case format in target records, add an optional CaseMapper argument to AvroType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.avro._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval avroType = AvroType[LowerCamel](CaseMapper(toSnakeCase))\navroType.to(LowerCamel(\"John\", \"Doe\"))\nAvro decimal and uuid logical types map to BigDecimal and java.util.UUID. Additionally decimal requires precision and optional scale parameter.\nimport magnolify.avro._\n\nimplicit val afBigDecimal: AvroField[BigDecimal] = AvroField.bigDecimal(20, 4)\nAmong the date/time types, date maps to java.time.LocalDate. The other types, timestamp, time and local-timestamp, map to Instant, LocalTime and LocalDateTime in either micro or milliseconds precision with import magnolify.avro.logical.micros._ or import magnolify.avro.logical.millis._.\nMap logical types to BigQuery compatible Avro with import magnolify.avro.logical.bigquery._.","title":"Avro"},{"location":"beam.html","text":"","title":"Beam"},{"location":"beam.html#beam","text":"RowType[T] provides conversion between Scala type T and a Beam Row, backed by a Beam Schema. Custom support for type T can be added with an implicit instance of RowField[T].\nimport java.net.URI\n\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.beam.*\n// Encode custom type URI as String\nimplicit val uriField: RowField[URI] = RowField.from[String](URI.create)(_.toString)\n\nval rowType = RowType[Outer]\nval row = rowType.to(record)\nval copy: Outer = rowType.from(row)\n\n// Beam Schema\nval schema = rowType.schema","title":"Beam"},{"location":"beam.html#enums","text":"Enum-like types map to the Beam logical Enum type. See EnumType for more details. UnsafeEnum[T] instances are available from import magnolify.beam.unsafe.*.","title":"Enums"},{"location":"beam.html#time-and-dates","text":"Java and joda LocalDate types are available via import magnolify.beam.logical.date.*\nFor date-time, instants, and durations, use import magnolify.beam.logical.millis.*, import magnolify.beam.logical.micros.* or import magnolify.beam.logical.nanos.* as appropriate for your use-case. Note that joda types have only millisecond resolution, so excess precision will be discarded when used with micros or nanos.\nWhere possible, Beam logical types are used and joda types defer to these implementations:\nBeam’s DATETIME primitive type maps to the millisecond-precision java and joda Instants and the joda DateTime. The DateTime logical type is used for millisecond-precision java and joda LocalDateTime The NanosInstant logical type is used for nanosecond-precision java and joda Instant The Time logical type is used for nanosecond-precision java and joda LocalTime The NanosDuration logical type is used for java and joda Duration\nBeam’s MicrosInstant should not be used as it throws exceptions when presented with greater-than-microsecond precision data.","title":"Time and dates"},{"location":"beam.html#sql-types","text":"SQL-compatible logical types are supported via import magnolify.beam.logical.sql.*","title":"SQL types"},{"location":"beam.html#case-mapping","text":"To use a different field case format in target records, add an optional CaseMapper argument to RowType:\nimport magnolify.beam.*\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval rowType = RowType[LowerCamel](CaseMapper(toSnakeCase))\nrowType.to(LowerCamel(\"John\", \"Doe\")) // Row(first_name: John, last_name: Doe)","title":"Case mapping"},{"location":"bigquery.html","text":"","title":"BigQuery"},{"location":"bigquery.html#bigquery","text":"TableRowType[T] provides conversion between Scala type T and BigQuery TableRow. Custom support for type T can be added with an implicit instance of TableRowField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.bigquery._\nimport com.google.api.services.bigquery.model.TableRow\n\n// Encode custom type URI as String\nimplicit val uriField: TableRowField[URI] = TableRowField.from[String](URI.create)(_.toString)\n\nval tableRowType = TableRowType[Outer]\nval tableRow: TableRow = tableRowType.to(record)\nval copy: Outer = tableRowType.from(tableRow)\n\n// BigQuery TableSchema\nval schema = tableRowType.schema\nAdditional TableRowField[T] instances for Byte, Char, Short, Int, Float, and enum-like types are available from import magnolify.bigquery.unsafe._. These conversions are unsafe due to potential overflow or encoding errors. See EnumType for more details.\nTo populate BigQuery table and field descriptions, annotate the case class and its fields with the @description annotation.\nimport magnolify.bigquery._\n\n@description(\"My record\")\ncase class Record(@description(\"int field\") i: Int, @description(\"string field\") s: String)\nThe @description annotation can also be extended to support custom format.\nimport magnolify.bigquery._\n\nclass myDesc(description: String, version: Int)\n  extends description(s\"description: $description, version: $version\")\n\n@myDesc(\"My record\", 2)\ncase class Record(@myDesc(\"int field\", 1) i: Int, @myDesc(\"string field\", 2) s: String)\nTo use a different field case format in target records, add an optional CaseMapper argument to TableRowType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.bigquery._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval tableRowType = TableRowType[LowerCamel](CaseMapper(toSnakeCase))\ntableRowType.to(LowerCamel(\"John\", \"Doe\"))","title":"BigQuery"},{"location":"bigtable.html","text":"","title":"Bigtable"},{"location":"bigtable.html#bigtable","text":"BigtableType[T] provides conversion between Scala type T and Bigtable Row/Seq[Mutation] for read/write. Custom support for type T can be added with an implicit instance of BigtableField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.bigtable._\nimport com.google.bigtable.v2.{Mutation, Row}\nimport com.google.protobuf.ByteString\n\n// Encode custom type URI as String\nimplicit val uriField: BigtableField[URI] = BigtableField.from[String](URI.create)(_.toString)\n\nval bigtableType = BigtableType[Outer]\nval mutations: Seq[Mutation] = bigtableType(record, \"ColumnFamily\")\nval row: Row = BigtableType.mutationsToRow(ByteString.copyFromUtf8(\"RowKey\"), mutations)\nval copy: Outer = bigtableType(row, \"ColumnFamily\")\nBigtableType encodes each field in a separate column qualifier of the same name. It encodes nested fields by joining field names as field_a.field_b.field_c. Repeated types are not supported. Enum-like types map to strings. See EnumType for more details.\nTo use a different field case format in target records, add an optional CaseMapper argument to BigtableType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.bigtable._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval bigtableType = BigtableType[LowerCamel](CaseMapper(toSnakeCase))\nbigtableType(LowerCamel(\"John\", \"Doe\"), \"cf\")","title":"Bigtable"},{"location":"cats.html","text":"","title":"Cats"},{"location":"cats.html#cats","text":"Type class derivation for Cats can be performed both automatically and semi-automatically.\nAutomatic derivation are provided as implicits through import magnolify.cats.auto._.\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\n// Cats Semigroup\nimport magnolify.cats.auto._\nimport cats._\n\nval sg: Semigroup[Outer] = implicitly[Semigroup[Outer]]\nsg.combine(Outer(Inner(1, \"hello, \")), Outer(Inner(100, \"world!\"))) // = Outer(Inner(101,hello, world!))\nSome Algebird instances extend those from Cats and can be derived as well.\nimport magnolify.cats.auto._\nimport com.twitter.{algebird => a}\n\ncase class Record(i: Int, o: Option[Int], l: List[Int], s: Set[Int], m: Map[String, Int])\n\n// implicit conversion from cats.{Semigroup,Monoid} to com.twitter.algebird.{Semigroup,Monoid}\nval sg: a.Semigroup[Record] = implicitly[a.Semigroup[Record]]\nval mon: a.Monoid[Record] = implicitly[a.Monoid[Record]]\nSemi-automatic derivation needs to be called explicitly.\nimport magnolify.cats.semiauto._\nimport cats._\n\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\nval eq: Eq[Outer] = EqDerivation[Outer]\nval hash: Hash[Outer] = HashDerivation[Outer]\nval sg: Semigroup[Outer] = SemigroupDerivation[Outer]\nval mon: Monoid[Outer] = MonoidDerivation[Outer]\n\n// this fails due to missing `Group[String]` instance\nval group: Group[Outer] = GroupDerivation[Outer]","title":"Cats"},{"location":"datastore.html","text":"","title":"Datastore"},{"location":"datastore.html#datastore","text":"EntityType[T] provides conversion between Scala type T and Datastore Entity. Custom support for type T can be added with an implicit instance of EntityField[T].\nimport java.net.URI\n\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.datastore._\nimport com.google.datastore.v1.Entity\n\n// Encode custom type URI as String\nimplicit val uriField: EntityField[URI] = EntityField.from[String](URI.create)(_.toString)\n\nval entityType = EntityType[Outer]\nval entityBuilder: Entity.Builder = entityType.to(record)\nval copy: Outer = entityType.from(entityBuilder.build)\nAdditional EntityField[T] instances for Byte, Char, Short, Int, Float, and enum-like types are available from import magnolify.datastore.unsafe._. These conversions are unsafe due to potential overflow or encoding errors. Enum like types map to strings. See EnumType for more details.\nTo set a field as key, annotate the field with key annotation.\nimport magnolify.datastore._\nimport com.google.protobuf.ByteString\n\n// Leave projectId as empty, use current package as namespaceId and class name \"Record\" as kind\ncase class Record(@key k: String, v: Long)\n\n// Custom projectId, namespaceId and kind.\ncase class ExplicitRecord(@key(\"my-project\", \"com.spotify\", \"MyKind\") k: String, v: Long)\n\n// Encode custom key type ByteString as String\ncase class ByteStringKey(@key k: ByteString)\nimplicit val kfByteString: KeyField[ByteString] = KeyField.at[ByteString](_.toStringUtf8)\n\n// Encode custom key type RecordKey as String\ncase class RecordKey(s: String, l: Long)\ncase class NestedKey(@key k: RecordKey)\nimplicit val kfRecord: KeyField[RecordKey] = KeyField.at[RecordKey](r => r.s + r.l)\nTo exclude a property from indexes, annotate the field with excludeFromIndexes annotation.\nimport magnolify.datastore._\ncase class Record(@excludeFromIndexes i: Int, @excludeFromIndexes(true) s: String)\nTo use a different field case format in target records, add an optional CaseMapper argument to EntityType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.datastore._\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval entityType = EntityType[LowerCamel](CaseMapper(toSnakeCase))\nentityType.to(LowerCamel(\"John\", \"Doe\"))","title":"Datastore"},{"location":"guava.html","text":"","title":"Guava"},{"location":"guava.html#guava","text":"Type class derivation for Guava can be performed both automatically and semi-automatically.\nAutomatic derivation are provided as implicits through import magnolify.guava.auto._.\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\n// Guava Funnel\nimport magnolify.guava.auto._ // includes implicit instances for Funnel[Int], etc.\nimport com.google.common.hash._\n\nval fnl: Funnel[Outer] = implicitly[Funnel[Outer]]\nval bf: BloomFilter[Outer] = BloomFilter.create[Outer](fnl, 1000)\nSemi-automatic derivation needs to be called explicitly.\nimport magnolify.guava.semiauto._\nimport com.google.common.hash._\n\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\nval fnl: Funnel[Outer] = FunnelDerivation[Outer]","title":"Guava"},{"location":"neo4j.html","text":"","title":"Neo4j"},{"location":"neo4j.html#neo4j","text":"TODO","title":"Neo4j"},{"location":"parquet.html","text":"","title":"Parquet"},{"location":"parquet.html#parquet","text":"ParquetType[T] provides read and write support between Scala type T and the Parquet columnar storage format. Custom support for type T can be added with an implicit instance of ParquetField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.parquet._\n\n// Encode custom type URI as String\nimplicit val uriField: ParquetField[URI] = ParquetField.from[String](b => URI.create(b))(_.toString)\n\nval parquetType = ParquetType[Outer]\n\n// Parquet schema\nval schema = parquetType.schema\nUse ParquetType#readBuilder and ParquetType#writeBuilder to create new file reader and writer instances. See HadoopSuite.scala for examples with Hadoop IO.","title":"Parquet"},{"location":"parquet.html#case-mapping","text":"To use a different field case format in target records, add an optional CaseMapper argument to ParquetType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\nimport magnolify.parquet._\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval parquetType = ParquetType[LowerCamel](CaseMapper(toSnakeCase))","title":"Case Mapping"},{"location":"parquet.html#enums","text":"Enum-like types map to strings. See EnumType for more details. Additional ParquetField[T] instances for Char and UnsafeEnum[T] are available from import magnolify.parquet.unsafe._. This conversions is unsafe due to potential overflow.","title":"Enums"},{"location":"parquet.html#logical-types","text":"Parquet decimal logical type maps to BigDecimal and supports the following encodings:\nimport magnolify.parquet._\n\nval pfDecimal32 = ParquetField.decimal32(9, 0)\nval pfDecimal64 = ParquetField.decimal64(18, 0)\nval pfDecimalFixed = ParquetField.decimalFixed(8, 18, 0)\nval pfDecimalBinary = ParquetField.decimalBinary(20, 0)\nFor a full specification of Date/Time mappings in Parquet, see Type Mappings.","title":"Logical Types"},{"location":"parquet.html#parquet-avro-compatibility","text":"The official Parquet format specification supports multiple valid schema representations of LIST types. Historically, magnolify-parquet has supported the simplest representation: simply marking the list element field as REPEATED, which per the spec defaults to a required list field with required elements. For example:\nimport magnolify.parquet._\n\ncase class RecordWithList(listField: List[Int])\nParquetType[RecordWithList].schema\n// res3: org.apache.parquet.schema.MessageType = message repl.MdocSession.MdocApp.RecordWithList {\n//   repeated int32 listField (INTEGER(32,true));\n// }\n//\nUnfortunately, this schema isn’t interoperable out-of-the-box with Parquet files produced by parquet-avro, which defaults to Parquet’s 2-level list encoding (with a configurable option to use 3-level list encoding).\nimport org.apache.avro.Schema\n\n// Avro schema matches `RecordWithList`\nval avroSchema = new Schema.Parser().parse(s\"\"\"{\n  \"type\": \"record\",\n  \"name\": \"RecordWithList\",\n  \"fields\": [\n    {\"name\": \"listField\", \"type\": {\"type\": \"array\", \"items\": \"int\"}}\n  ]\n}\"\"\")\n// avroSchema: Schema = {\"type\":\"record\",\"name\":\"RecordWithList\",\"fields\":[{\"name\":\"listField\",\"type\":{\"type\":\"array\",\"items\":\"int\"}}]}\n\n// Used by parquet-avro to convert Avro to Parquet schemas\nimport org.apache.parquet.avro.AvroSchemaConverter\n\n// 2-level list encoding -- compare to schema generated for `RecordWithList` above\nval convertedParquetSchema = new AvroSchemaConverter().convert(avroSchema)\n// convertedParquetSchema: org.apache.parquet.schema.MessageType = message RecordWithList {\n//   required group listField (LIST) {\n//     repeated int32 array;\n//   }\n// }\n//\nParquet-avro doesn’t fully support the spec magnolify-parquet adheres to and can’t interpret the Magnolify list schema. As a result, by default, if your schema contains a repeated type, records produced by parquet-avro can’t be consumed by magnolify-parquet, and vice versa, unless you’re using Parquet-Avro Compatibility Mode.","title":"Parquet-Avro Compatibility"},{"location":"parquet.html#parquet-avro-compatibility-mode","text":"When Parquet-Avro Compatibility Mode is enabled, magnolify-parquet will interpret repeated fields using the same 2-level list structure that parquet-avro uses. In addition, Parquet file writes will include an extra metadata key, parquet.avro.schema, to the file footer, containing the converted, String-serialized Avro schema.","title":"Parquet-Avro Compatibility Mode"},{"location":"parquet.html#enabling-compatibility-mode-on-magnolify-0-8","text":"You can enable this mode by importing magnolify.parquet.ParquetArray.AvroCompat._ at the site where your ParquetType[T] is derived. Note that you’ll need to add this import for both writes (to produce 2-level encoded lists) and reads (to consume 2-level encoded lists).\nimport magnolify.parquet.ParquetArray.AvroCompat._\n\ncase class RecordWithList(listField: List[String])\n\nval pt = ParquetType[RecordWithList]\n// error: parquetType is already defined as value parquetType\n// error: RecordWithList is already defined as case class RecordWithList","title":"Enabling Compatibility Mode on Magnolify < 0.8"},{"location":"parquet.html#enabling-compatibility-mode-on-magnolify-0-8","text":"The magnolify.parquet.ParquetArray.AvroCompat._ import is deprecated in Magnolify 0.8 and is expected to be removed in future versions.\nInstead, in Magnolify 0.8 and above, this mode should be enabled on the writer by setting a Hadoop Configuration option, magnolify.parquet.write-grouped-arrays.\nimport org.apache.hadoop.conf.Configuration\nimport magnolify.parquet._\n\ncase class RecordWithList(listField: List[String])\n\nval conf = new Configuration()\n// conf: Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-rbf-default.xml, hdfs-site.xml, hdfs-rbf-site.xml\nconf.setBoolean(MagnolifyParquetProperties.WriteAvroCompatibleArrays, true) // sets `magnolify.parquet.write-grouped-arrays`\n\n// Instantiate ParquetType with configuration\nval pt = ParquetType[RecordWithList](conf)\n// pt: ParquetType[RecordWithList] = magnolify.parquet.ParquetType$$anon$2@29fac605\n\n// Check that the converted Avro schema uses 2-level encoding\npt.schema\n// res7: org.apache.parquet.schema.MessageType = message repl.MdocSession.MdocApp5.RecordWithList {\n//   required group listField (LIST) {\n//     repeated binary array (STRING);\n//   }\n// }\n//\nIf you’re a Scio user with com.spotify:scio-parquet on your classpath, you can instantiate a Configured ParqueType as a one-liner:\nimport com.spotify.scio.parquet._\nimport magnolify.parquet._\n\ncase class RecordWithList(listField: List[String])\n\nval pt = ParquetType[RecordWithList](\n  ParquetConfiguration.of(MagnolifyParquetProperties.WriteAvroCompatibleArrays -> true)\n)\n// error: parquetType is already defined as value parquetType\n// error: RecordWithList is already defined as case class RecordWithList\n// error: pt is already defined as value pt\n// error: object spotify is not a member of package com\n// error: not found: value ParquetConfiguration\n//   ParquetConfiguration.of(MagnolifyParquetProperties.WriteAvroCompatibleArrays -> true)\n//   ^^^^^^^^^^^^^^^^^^^^\nYou can combine a Configuration with a CaseMapper:\nimport magnolify.shared._\n\n// Can be combined with a CaseMapper\nval cm: CaseMapper = ???\nParquetType[RecordWithList](cm, conf)\nIf you don’t have Hadoop on your classpath, you can instantiate a MagnolifyParquetProperties instance directly:\nimport magnolify.parquet._\n\nParquetType[RecordWithList](new MagnolifyParquetProperties {\n    override def WriteAvroCompatibleArrays: Boolean = true\n  }\n)\nOn the reader side, 2-level arrays will be detected automatically based on the input file schema, so no imports or extra Configurations are needed.","title":"Enabling Compatibility Mode on Magnolify >= 0.8"},{"location":"parquet.html#field-descriptions","text":"The top level class and all fields (including nested class fields) can be annotated with @doc annotation. Note that nested classes annotations are ignored.\nimport magnolify.shared._\n\n@doc(\"This is ignored\")\ncase class NestedClass(@doc(\"nested field annotation\") i: Int)\n\n@doc(\"Top level annotation\")\ncase class TopLevelType(@doc(\"field annotation\") pd: NestedClass)\nNote that field descriptions are not natively supported by the Parquet format. Instead, the @doc annotation ensures that the generated Avro schema written to the metadata key parquet.avro.schema will contain the specified field description:\nimport magnolify.parquet._\nimport magnolify.shared._\n\n@doc(\"Top level annotation\")\ncase class MyRecord(@doc(\"field annotation\") listField: List[Int])\n\n// Note: If using Magnolify < 0.8, import magnolify.parquet.ParquetArray.AvroCompat._\n// to ensure `parquet.avro.schema` metadata is written to file footer\nval writer = ParquetType[MyRecord]\n  .writeBuilder(HadoopOutputFile.fromPath(path, new Configuration()))\n  .build()\n// writer: org.apache.parquet.hadoop.ParquetWriter[MyRecord] = org.apache.parquet.hadoop.ParquetWriter@60a12960\nwriter.write(MyRecord(List(1,2,3)))\nwriter.close()\n\n// Note that Parquet MessageType schema doesn't contain descriptor, but serialized Avro schema does\nParquetFileReader.open(HadoopInputFile.fromPath(path, new Configuration())).getFileMetaData\n// res14: org.apache.parquet.hadoop.metadata.FileMetaData = FileMetaData{schema: message repl.MdocSession.MdocApp11.MyRecord {\n//   repeated int32 listField (INTEGER(32,true));\n// }\n// , metadata: {parquet.avro.schema={\"type\":\"record\",\"name\":\"MyRecord\",\"namespace\":\"repl.MdocSession.MdocApp11\",\"doc\":\"Top level annotation\",\"fields\":[{\"name\":\"listField\",\"type\":{\"type\":\"array\",\"items\":\"int\"},\"doc\":\"field annotation\",\"default\":[]}]}, writer.model.name=magnolify}}\nNote: On Magnolify < 0.8, you must enable Avro compatibility mode via the AvroCompat import if you’re using the @doc annotation with ParquetType, which triggers magnolify-parquet to write a translated Avro schema to the file footer metadata key parquet.avro.schema. Otherwise, your annotations will be essentially thrown out. On Magnolify >= 0.8, this key is written by default.","title":"Field Descriptions"},{"location":"protobuf.html","text":"","title":"Protobuf"},{"location":"protobuf.html#protobuf","text":"ProtobufType[T, MsgT] provides conversion between Scala type T and Protobuf MsgT <: Message. Custom support for type T can be added with an implicit instance of ProtobufField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\n// Protobuf record\nabstract class MyProto extends com.google.protobuf.Message\n\nimport magnolify.protobuf._\n\n// Encode custom type URI as String\nimplicit val uriField = ProtobufField.from[String](URI.create)(_.toString)\n \n// MyProto is a compiled Protobuf Message\nval protobufType = ProtobufType[Outer, MyProto]\nval proto: MyProto = protobufType.to(record)\nval copy: Outer = protobufType.from(proto)\nEnum like types map to Protobuf enums. See EnumType for more details. An implicit instance from Java or Scala type to Protobuf enum must be provided.\n// Scala enum\nobject Color extends Enumeration {\n  type Type = Value\n  val Red, Green, Blue = Value\n}\n\n// Protobuf enum\n// enum ColorProto {\n//   RED = 0;\n//   GREEN = 1;\n//   BLUE = 2;\n// }\nabstract class ColorProto(name: String, ordinal: Int) extends Enum[ColorProto](name, ordinal) with com.google.protobuf.ProtocolMessageEnum\n\nimport magnolify.protobuf._\nimplicit val efEnum = ProtobufField.enum[Color.Type, ColorProto]\nAdditional ProtobufField[T] instances for Byte, Char, Short, and UnsafeEnum[T] are available from import magnolify.protobuf.unsafe._. These conversions are unsafe due to potential overflow.\nTo use a different field case format in target records, add an optional CaseMapper argument to ProtobufType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\nimport magnolify.protobuf._\n\ncase class LowerCamel(firstName: String, lastName: String)\n\n// Protobuf record\nabstract class LowerHyphenProto extends com.google.protobuf.Message\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval protobufType = ProtobufType[LowerCamel, LowerHyphenProto](CaseMapper(toSnakeCase))\nprotobufType.to(LowerCamel(\"John\", \"Doe\"))","title":"Protobuf"},{"location":"refined.html","text":"","title":"Refined"},{"location":"refined.html#refined","text":"Refined is a Scala library for refining types with type-level predicates which constrain the set of values described by the refined type.\nimport eu.timepit.refined._\nimport eu.timepit.refined.api._\nimport eu.timepit.refined.auto._\nimport eu.timepit.refined.numeric._\nimport eu.timepit.refined.string._\n\ncase class Record(pos: Int Refined Positive, url: String Refined Url)\n\n// Refinements are checked at compile time with literals\nRecord(42, \"https://www.spotify.com\")\n// otherwise unsafe runtime check have to be used for variables.\nval x = -1\nval url = \"foo\"\n// Throws IllegalArgumentException\nRecord(refineV.unsafeFrom(x), refineV.unsafeFrom(url))\nMagnolify works with Refined through some extra implicits.\nimport eu.timepit.refined.api._\nimport eu.timepit.refined.auto._\nimport eu.timepit.refined.numeric._\nimport eu.timepit.refined.string._\n\nimport magnolify.avro._\nimport magnolify.refined.avro._\n\ncase class Record(pos: Int Refined Positive, url: String Refined Url)\n\nval at = AvroType[Record]\nat(Record(42, \"https://www.spotify.com\"))","title":"Refined"},{"location":"scalacheck.html","text":"","title":"Scalacheck"},{"location":"scalacheck.html#scalacheck","text":"Type class derivation for Scalacheck can be performed both automatically and semi-automatically.\nAutomatic derivation are provided as implicits through import magnolify.scalacheck.auto._.\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\n// ScalaCheck Arbitrary\nimport magnolify.scalacheck.auto._\nimport org.scalacheck._ // implicit instances for Arbitrary[Int], etc.\n\nval arb: Arbitrary[Outer] = implicitly[Arbitrary[Outer]]\narb.arbitrary.sample // = Some(Outer(Inter(12345, abcde)))\nSemi-automatic derivation needs to be called explicitly.\nimport magnolify.scalacheck.semiauto._\nimport org.scalacheck._\n\ncase class Inner(int: Int, str: String)\ncase class Outer(inner: Inner)\n\nval arb: Arbitrary[Outer] = ArbitraryDerivation[Outer]\nval cogen: Cogen[Outer] = CogenDerivation[Outer]","title":"Scalacheck"},{"location":"tensorflow.html","text":"","title":"Tensorflow"},{"location":"tensorflow.html#tensorflow","text":"ExampleType[T] provides conversion between Scala type T and TensorFlow Example. Custom support for type T can be added with an implicit instance of ExampleField[T].\nimport java.net.URI\ncase class Inner(long: Long, str: String, uri: URI)\ncase class Outer(inner: Inner)\nval record = Outer(Inner(1L, \"hello\", URI.create(\"https://www.spotify.com\")))\n\nimport magnolify.tensorflow._\nimport com.google.protobuf.ByteString\nimport org.tensorflow.proto.example.{Example, Example.Builder}\n\n// Encode custom type String/URI as ByteString\nimplicit val stringField = ExampleField.from[ByteString](_.toStringUtf8)(ByteString.copyFromUtf8)\nimplicit val uriField = ExampleField\n  .from[ByteString](b => URI.create(b.toStringUtf8))(u => ByteString.copyFromUtf8(u.toString))\n\nval exampleType = ExampleType[Outer]\nval exampleBuilder: Example.Builder = exampleType.to(record)\nval copy = exampleType.from(exampleBuilder.build)\nExampleType encodes each field in a Feature of the same name. It encodes nested fields by joining field names as field_a.field_b.field_c. Optional and repeated types are not supported in a nested field.\nAdditional ExampleField[T] instances for Byte, Char, Short, Int, Double, Boolean, String, and enum-like types are available from import magnolify.tensorflow.unsafe._. These conversions are unsafe due to potential overflow and encoding errors. Enum-like types map to strings. See EnumType for more details.\nTo use a different field case format in target records, add an optional CaseMapper argument to ExampleType. The following example maps firstName & lastName to first_name & last_name.\nimport magnolify.shared.CaseMapper\nimport com.google.common.base.CaseFormat\n\ncase class LowerCamel(firstName: String, lastName: String)\n\nval toSnakeCase = CaseFormat.LOWER_CAMEL.converterTo(CaseFormat.LOWER_UNDERSCORE).convert _\nval exampleType = ExampleType[LowerCamel](CaseMapper(toSnakeCase))\nexampleType.to(LowerCamel(\"John\", \"Doe\"))","title":"Tensorflow"},{"location":"enums.html","text":"","title":"EnumType"},{"location":"enums.html#enumtype","text":"EnumType[T] provides conversion between enum-like types and their string names. Supported enum-like types are Java enum, Scala Enumeration, and sealed trait with case objects. AvroType[T] and ProtobufType[T] use it to map to their native enum types, i.e. EnumSymbol and ProtocolMessageEnum, while other converters map to strings.\nCaseMapper supports enums too.\nobject Color extends Enumeration {\n  type Type = Value\n  val Red, Green, Blue = Value\n}\n\nimport magnolify.shared._\n// Encode as [\"red\", \"green\", \"blue\"]\nimplicit val enumType: EnumType[Color.Type] = EnumType.scalaEnumType[Color.Type].map(CaseMapper(_.toLowerCase))\n// enumType: EnumType[Color.Type] = magnolify.shared.EnumType$$anon$1@28cab4b1\nAn enum-like type can be wrapped inside a magnolify.shared.UnsafeEnum[T] to handle unknown cases. This could be useful for scenarios like schema evolution, or working with bad data.\nUnsafeEnum(Color.Red) // Known(Red)\n// res0: UnsafeEnum.Known[Color.Value] = Known(value = Red)\nUnsafeEnum.from[Color.Type](\"Red\") // Known(Red)\n// res1: UnsafeEnum[Color.Type] = Unknown(value = \"Red\")\nUnsafeEnum.from[Color.Type](\"Purple\") // Unknown(Purple)\n// res2: UnsafeEnum[Color.Type] = Unknown(value = \"Purple\")","title":"EnumType"},{"location":"mapping.html","text":"","title":"Type Mapping"},{"location":"mapping.html#type-mapping","text":"Scala Avro Beam BigQuery Bigtable7 Datastore Parquet Protobuf TensorFlow Unit null x x x Null x x x Boolean boolean BOOLEAN BOOL Byte Boolean BOOLEAN Boolean INT643 Char int3 BYTE INT643 Char Integer3 INT323 Int3 INT643 Byte int3 BYTE INT643 Byte Integer3 INT329 Int3 INT643 Short int3 INT16 INT643 Short Integer3 INT329 Int3 INT643 Int int INT32 INT643 Int Integer3 INT329 Int INT643 Long long INT64 INT64 Long Integer INT649 Long INT64 Float float FLOAT FLOAT643 Float Double3 FLOAT Float FLOAT Double double DOUBLE FLOAT64 Double Double DOUBLE Double FLOAT3 CharSequence string STRING x x x x x x String string STRING STRING String String BINARY String BYTES3 Array[Byte] bytes BYTES BYTES ByteString Blob BINARY ByteString BYTES ByteString x BYTES x ByteString Blob x ByteString BYTES ByteBuffer bytes BYTES x x x x x Enum1 enum STRING16 STRING3 String String3 BINARY/ENUM9 Enum BYTES3 BigInt x x x BigInt x x x x BigDecimal bytes4 DECIMAL NUMERIC6 Int scale + unscaled BigInt x LOGICAL[DECIMAL]9,14 x x Option[T] union[null, T]5 Empty as null NULLABLE Empty as None Absent as None OPTIONAL optional10 Size <= 1 Iterable[T]2 array[T] ITERABLE REPEATED x Array REPEATED13 repeated Size >= 0 Nested record ROW STRUCT Flat8 Entity Group Message Flat8 Map[K, V] map[V]15 MAP x x x x map<K, V> x java.time.Instant long11 DATETIME, INT64, ROW17 TIMESTAMP x Timestamp LOGICAL[TIMESTAMP]9 x x java.time.LocalDateTime long11 ROW, INT6417 DATETIME x x LOGICAL[TIMESTAMP]9 x x java.time.OffsetTime x x x x x LOGICAL[TIME]9 x x java.time.LocalTime long11 INT32, INT6417 TIME x x LOGICAL[TIME]9 x x java.time.LocalDate int11 INT6417 DATE x x LOGICAL[DATE]9 x x org.joda.time.LocalDate int11 INT6417 x x x x x x org.joda.time.DateTime int11 DATETIME, INT64, ROW17 x x x x x x org.joda.time.LocalTime int11 INT32, INT6417 x x x x x x java.util.UUID string4 ROW18 x ByteString (16 bytes) x FIXED[16] x x (Long, Long, Long)12 fixed[12] x x x x x x x\nThose wrapped inUnsafeEnum are encoded as strings, see enums.md for more Any subtype of Iterable[T] Unsafe conversions, import magnolify.$MODULE.unsafe._ Avro logical types (doc) UNION of [NULL, T] and defaults to NULL (doc) Fixed precision of 38 and scale of 9 (doc) All Scala types are encoded as big endian ByteString for Bigtable Nested fields are encoded flat with field names joined with ., e.g. level1.level2.level3 More information on Parquet logical type schemas can be found here. Time types are available at multiple precisions; import magnolify.parquet.logical.micros._, magnolify.avro.logical.millis._, or magnolify.avro.logical.nanos._ accordingly. See protobuf.md for more Logical types available at micro- or milli-second precision; import magnolify.avro.logical.micros._ or magnolify.avro.logical.millis._ accordingly. BigQuery-compatible conversions are available in magnolify.avro.logical.bigquery._. Special tuple used to represent Duration in the Avro spec. This has not been made implicit in Magnolify; import AvroType.afDuration implicitly to enable If magnolify.parquet.ParquetArray.AvroCompat._ is imported, array fields use the nested, Avro-compatible schema format: required group $FIELDNAME (LIST) { repeated $FIELDTYPE array ($FIELDSCHEMA); }. Parquet’s Decimal logical format supports multiple representations, and are not implicitly scoped by default. Import one of: magnolify.parquet.ParquetField.{decimal32, decimal64, decimalFixed, decimalBinary}. Map key type in avro is fixed to string. Scala Map key type must be either String or CharSequence. Beam logical Enumeration type See beam.md for details Beam logical UUID type","title":"Type Mapping"},{"location":"scaladoc.html","text":"","title":""}]}